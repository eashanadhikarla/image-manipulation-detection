{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Name    : Eashan Adhikarla\n",
    "Subject : Media Forensics\n",
    "Project : Mini-project-2 (Task 2 - Bonus)\n",
    "Data    : April 10, 2021\n",
    "==============================================\n",
    "\n",
    "\"\"\"\n",
    "# --- Sklearn ---\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import decomposition, discriminant_analysis, linear_model, svm, tree, neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Models ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# --- Utility ---\n",
    "import os, cv2, glob\n",
    "import pickle\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prepareData import AzimuthalAverage\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Dropbox (LU Student)/Macbook/Desktop/Media Forensics/mini-project-2/Task 2/\"\n",
    "rootdir = \"/data/MediaForensics/DeepFake/Frequency/\"\n",
    "\n",
    "\n",
    "\n",
    "trainpath = ['Faces-HQ/100KFake_10K','Faces-HQ/celebA-HQ_10K']\n",
    "trainlabels = [1, 0]\n",
    "traindatadir = \"/home/jupyter-eaa418/MediaForensics/Task 2/data/Bonus/FacesHQ_traindata.pkl\"\n",
    "\n",
    "testpath = ['Faces-HQ/thispersondoesntexists_10K','Faces-HQ/Flickr-Faces-HQ_10K']\n",
    "testlabels = [1, 0]\n",
    "testdatadir = \"/home/jupyter-eaa418/MediaForensics/Task 2/data/Bonus/FacesHQ_testdata.pkl\"\n",
    "\n",
    "epsilon = 1e-8\n",
    "Data = {}\n",
    "classes = ('Real', 'Fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of samples from each dataset\n",
    "# stop = 2000\n",
    "# z, iter_ = 0, 0\n",
    "\n",
    "# number_iter = 2 * stop\n",
    "# Azimuthalavg1D = np.zeros([number_iter, 722])\n",
    "# label_total = np.zeros([number_iter])\n",
    "\n",
    "\n",
    "# for data in range(len(trainpath)):\n",
    "#     dataIdxCount = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "#     print(f\"Processing dataset {trainpath[data]}...\")\n",
    "\n",
    "\n",
    "# for z in range(len(trainpath)):\n",
    "#     cont = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "    \n",
    "#     for filename in glob.glob(str(rootdir)+trainpath[data]+\"/*.jpg\"):  \n",
    "#         # print(filename)\n",
    "#         img = cv2.imread(filename,0)\n",
    "        \n",
    "#         f = np.fft.fft2(img)\n",
    "#         fshift = np.fft.fftshift(f)\n",
    "#         fshift += epsilon\n",
    "        \n",
    "#         magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "\n",
    "#         # Calculate the azimuthally averaged 1D power spectrum\n",
    "#         psd1D = AzimuthalAverage(magnitude_spectrum)\n",
    "#         Azimuthalavg1D[iter_,:] = psd1D\n",
    "#         label_total[iter_] = trainlabels[z]\n",
    "\n",
    "#         cont += 1\n",
    "#         iter_ += 1\n",
    "#         if cont >= stop:\n",
    "#             break\n",
    "\n",
    "# Data[\"data\"], Data[\"label\"] = Azimuthalavg1D, label_total\n",
    "# print(len(label_total))\n",
    "\n",
    "# output = open(traindatadir, 'wb')\n",
    "# pickle.dump(Data, output)\n",
    "# output.close()\n",
    "# print(\"Train data preprocessed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of samples from each dataset\n",
    "# stop = 250\n",
    "# z, iter_ = 0, 0\n",
    "\n",
    "# number_iter = 2 * stop\n",
    "# Azimuthalavg1D = np.zeros([number_iter, 722])\n",
    "# label_total = np.zeros([number_iter])\n",
    "\n",
    "\n",
    "# for data in range(len(testpath)):\n",
    "#     dataIdxCount = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "#     print(f\"Processing dataset {testpath[data]}...\")\n",
    "\n",
    "\n",
    "# for z in range(len(testpath)):\n",
    "#     cont = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "    \n",
    "#     for filename in glob.glob(str(rootdir)+testpath[data]+\"/*.jpg\"):  \n",
    "#         # print(filename)\n",
    "#         img = cv2.imread(filename,0)\n",
    "        \n",
    "#         f = np.fft.fft2(img)\n",
    "#         fshift = np.fft.fftshift(f)\n",
    "#         fshift += epsilon\n",
    "        \n",
    "#         magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "\n",
    "#         # Calculate the azimuthally averaged 1D power spectrum\n",
    "#         psd1D = AzimuthalAverage(magnitude_spectrum)\n",
    "#         Azimuthalavg1D[iter_,:] = psd1D\n",
    "#         label_total[iter_] = testlabels[z]\n",
    "\n",
    "#         cont += 1\n",
    "#         iter_ += 1\n",
    "#         if cont >= stop:\n",
    "#             break\n",
    "\n",
    "# Data[\"data\"], Data[\"label\"] = Azimuthalavg1D, label_total\n",
    "# print(len(label_total))\n",
    "\n",
    "# output = open(testdatadir, 'wb')\n",
    "# pickle.dump(Data, output)\n",
    "# output.close()\n",
    "# print(\"Test data preprocessed and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label=0 : 1899\n",
      "Train label=1 : 1901\n",
      "Test label=0 : 250\n",
      "Test label=1 : 250\n"
     ]
    }
   ],
   "source": [
    "# read python dict back from the file\n",
    "pkl_file = open(traindatadir, 'rb')\n",
    "traindata = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(testdatadir, 'rb')\n",
    "testdata = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "X_train, Y_train = traindata[\"data\"], traindata[\"label\"]\n",
    "X_test, Y_test = testdata[\"data\"], testdata[\"label\"]\n",
    "\n",
    "print(f\"Total train images: {len(X_train)}, Total train labels: {len(Y_train)}\")\n",
    "print(f\"Total test images: {len(X_test)}, Total test labels: {len(Y_test)}\")\n",
    "\n",
    "# ======================================================================\n",
    "\n",
    "# # Random Split\n",
    "# X_train, _, Y_train, _ = train_test_split(traindata[\"data\"], traindata[\"label\"], test_size = 0.05)\n",
    "\n",
    "# # Stratified Split\n",
    "# stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# stratSplit.get_n_splits(images, labels)\n",
    "\n",
    "# for train_index, test_index in stratSplit.split(images, labels):\n",
    "#     X_train, X_test = images[train_index], images[test_index]\n",
    "#     Y_train, Y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "# print(\"Training data shape :\", X_train.shape, Y_train.shape)\n",
    "# print(\"Training data shape :\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Train label=0 :\", np.count_nonzero(Y_train==0))\n",
    "print(\"Train label=1 :\", np.count_nonzero(Y_train==1))\n",
    "print(\"Test label=0 :\",  np.count_nonzero(Y_test==0))\n",
    "print(\"Test label=1 :\",  np.count_nonzero(Y_test==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    \n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def SupportVectorMachine(train, save, test):\n",
    "    # filename = \"/home/jupyter-eaa418/MediaForensics/Task 2/checkpoint/Bonus/svcBest.pt\"\n",
    "    filename = \"/home/jupyter-eaa418/MediaForensics/Task 2/checkpoint/svcBest.pt\"\n",
    "    svc = svm.SVC(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"kernel\":('linear', 'rbf'), \n",
    "                \"C\":[1, 10, 500]\n",
    "                }\n",
    "\n",
    "        svcBest = train_and_tune(X_train,\n",
    "                                 Y_train,\n",
    "                                 svc,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "        if save:\n",
    "            save_model(filename, svcBest)\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        svcBest_reloaded = load_model(filename)\n",
    "        pred = svcBest_reloaded.predict(X_test)\n",
    "        acc  = svcBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix)*10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method-SVC completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "\n",
    "def MultiLayerPerceptron(train, save, test):\n",
    "    filename = \"/home/jupyter-eaa418/MediaForensics/Task 2/checkpoint/mlpBest.pt\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\n",
    "                   \"alpha\" : [0.0001],\n",
    "                   \"learning_rate_init\" : [0.005],\n",
    "                   \"batch_size\" : [8, 32, 64, 128],\n",
    "                   \"activation\" : [\"relu\"],\n",
    "                   \"early_stopping\" : [True],\n",
    "                   \"hidden_layer_sizes\" : [3, 10, 50, 100],\n",
    "                 }\n",
    "\n",
    "        mlpBest = train_and_tune(X_train,\n",
    "                                Y_train,\n",
    "                                mlp,\n",
    "                                params,\n",
    "                                scoring='f1_macro',\n",
    "                                kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X_test)\n",
    "        acc  = mlpBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method MLP completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "\n",
    "def KNearestNeighbors(train, save, test):\n",
    "    filename = \"/home/jupyter-eaa418/MediaForensics/Task 2/checkpoint/knnclassifierBest.pkl\"\n",
    "    knnclassifier = KNeighborsClassifier()\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"n_neighbors\": [2, 6, 8, 10],\n",
    "                }\n",
    "\n",
    "        knnclassifierBest = train_and_tune(X_train,\n",
    "                                Y_train,\n",
    "                                knnclassifier,\n",
    "                                params,\n",
    "                                scoring='f1_macro',\n",
    "                                kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, knnclassifierBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        knnclassifierBest_reloaded = load_model(filename)\n",
    "        pred = knnclassifierBest_reloaded.predict(X_test)\n",
    "        acc  = knnclassifierBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        # cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        # df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "        #                      columns = [i for i in classes])\n",
    "        # plt.figure(figsize = (12,10))\n",
    "        # sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method-KNN completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "svm_acc = SupportVectorMachine(train=True, save=True, test=True)\n",
    "mlp_acc = MultiLayerPerceptron(train=True, save=True, test=True)\n",
    "knn_acc = KNearestNeighbors(train=True, save=True, test=True)\n",
    "# svm_acc = SupportVectorMachine(train=False, save=False, test=True)\n",
    "# mlp_acc = MultiLayerPerceptron(train=False, save=False, test=True)\n",
    "# knn_acc = KNearestNeighbors(train=False, save=False, test=True)\n",
    "\n",
    "print(\"=\"*25)\n",
    "print(\"MODEL ARCH.\\t ACCURACY\")\n",
    "print(\"-\"*25)\n",
    "print(\"SVM\\t\\t  \", svm_acc)\n",
    "print(\"-\"*25)\n",
    "print(\"MLP\\t\\t   \", mlp_acc)\n",
    "print(\"-\"*25)\n",
    "print(\"knn_acc\\t\\t  \", knn_acc)\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
