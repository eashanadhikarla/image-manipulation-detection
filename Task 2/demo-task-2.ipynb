{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Name    : Eashan Adhikarla\n",
    "Subject : Media Forensics\n",
    "Project : Mini-project-2 (Task 2)\n",
    "Data    : April 10, 2021\n",
    "==============================================\n",
    "\n",
    "\"\"\"\n",
    "# --- Sklearn ---\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import decomposition, discriminant_analysis, linear_model, svm, tree, neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Models ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# --- Utility ---\n",
    "import os, cv2, glob\n",
    "import pickle\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prepareData import AzimuthalAverage\n",
    "\n",
    "# rootdir = \"/Users/eashan22/Dropbox (LU Student)/Macbook/Desktop/Media Forensics/mini-project-2/Task 2/\"\n",
    "rootdir = \"/data/MediaForensics/DeepFake/Frequency/\"\n",
    "\n",
    "path    = ['Faces-HQ/thispersondoesntexists_10K',\n",
    "           'Faces-HQ/100KFake_10K',\n",
    "           'Faces-HQ/Flickr-Faces-HQ_10K',\n",
    "           'Faces-HQ/celebA-HQ_10K',\n",
    "           ]\n",
    "\n",
    "datadir = \"./data/FacesHQ_Data.pkl\"\n",
    "# datadir = \"./data/Data.pkl\"\n",
    "\n",
    "labels = [1, 1, 0, 0]\n",
    "epsilon = 1e-8\n",
    "Data = {}\n",
    "classes = ('Real', 'Fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of samples from each dataset\n",
    "# stop = 625\n",
    "# z, iter_ = 0, 0\n",
    "\n",
    "# number_iter = 4 * stop\n",
    "# Azimuthalavg1D = np.zeros([number_iter, 722])\n",
    "# label_total = np.zeros([number_iter])\n",
    "\n",
    "\n",
    "# for data in range(len(path)):\n",
    "#     dataIdxCount = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "#     print(f\"Processing dataset {path[data]}...\")\n",
    "\n",
    "\n",
    "# for z in range(len(path)):\n",
    "#     cont = 0\n",
    "#     psd1D_average_org = np.zeros(722)\n",
    "    \n",
    "#     for filename in glob.glob(str(rootdir)+path[data]+\"/*.jpg\"):  \n",
    "#         # print(filename)\n",
    "#         img = cv2.imread(filename,0)\n",
    "        \n",
    "#         f = np.fft.fft2(img)\n",
    "#         fshift = np.fft.fftshift(f)\n",
    "#         fshift += epsilon\n",
    "        \n",
    "#         magnitude_spectrum = 20*np.log(np.abs(fshift))\n",
    "\n",
    "#         # Calculate the azimuthally averaged 1D power spectrum\n",
    "#         psd1D = AzimuthalAverage(magnitude_spectrum)\n",
    "#         Azimuthalavg1D[iter_,:] = psd1D\n",
    "#         label_total[iter_] = labels[z]\n",
    "\n",
    "#         cont += 1\n",
    "#         iter_ += 1\n",
    "#         if cont >= stop:\n",
    "#             break\n",
    "\n",
    "# Data[\"data\"], Data[\"label\"] = Azimuthalavg1D, label_total\n",
    "# print(len(label_total))\n",
    "\n",
    "# output = open(datadir, 'wb')\n",
    "# pickle.dump(Data, output)\n",
    "# output.close()\n",
    "# print(\"Data Preprocessed and Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load feature file\n",
    "# pkl_file = open(datadir, 'rb')\n",
    "# Data = pickle.load(pkl_file)\n",
    "# pkl_file.close()\n",
    "# images, labels = Data[\"data\"], Data[\"label\"]\n",
    "\n",
    "# plt.plot(labels)\n",
    "# plt.show()\n",
    "\n",
    "# num = int(labels.shape[0]/2)\n",
    "# num_feat = images.shape[1]\n",
    "\n",
    "# psd1D_org_0 = np.zeros((num, num_feat))\n",
    "# psd1D_org_1 = np.zeros((num, num_feat))\n",
    "\n",
    "# psd1D_org_0_mean, psd1D_org_1_mean = np.zeros(num_feat), np.zeros(num_feat)\n",
    "# psd1D_org_0_std, psd1D_org_1_std   = np.zeros(num_feat), np.zeros(num_feat)\n",
    "\n",
    "# counter0, counter1 = 0, 0\n",
    "\n",
    "# # We separate real and fake using the label\n",
    "# for idx in range(images.shape[0]):\n",
    "#     if labels[idx]==0:\n",
    "#         psd1D_org_0[counter0,:] = images[idx,:]\n",
    "#         counter0+=1\n",
    "#     elif labels[idx]==1:\n",
    "#         psd1D_org_1[counter1,:] = images[idx,:]\n",
    "#         counter1+=1\n",
    "\n",
    "# # We compute statistcis\n",
    "# for x in range(num_feat):\n",
    "#     psd1D_org_0_mean[x] = np.mean(psd1D_org_0[:,x])\n",
    "#     psd1D_org_0_std[x]  = np.std (psd1D_org_0[:,x])\n",
    "#     psd1D_org_1_mean[x] = np.mean(psd1D_org_1[:,x])\n",
    "#     psd1D_org_1_std[x]  = np.std (psd1D_org_1[:,x])  \n",
    "\n",
    "# # Plot\n",
    "# x = np.arange(0, num_feat, 1)\n",
    "# fig, ax = plt.subplots(figsize=(15, 9))\n",
    "\n",
    "# ax.plot(x, psd1D_org_0_mean, alpha=0.5, color='red', label='Real', linewidth =2.0)\n",
    "# ax.fill_between(x, psd1D_org_0_mean - psd1D_org_0_std, psd1D_org_0_mean + psd1D_org_0_std, color='red', alpha=0.2)\n",
    "# ax.plot(x, psd1D_org_1_mean, alpha=0.5, color='blue', label='Fake', linewidth =2.0)\n",
    "# ax.fill_between(x, psd1D_org_1_mean - psd1D_org_1_std, psd1D_org_1_mean + psd1D_org_1_std, color='blue', alpha=0.2)\n",
    "\n",
    "# ax.legend()\n",
    "# plt.tick_params(axis='x', labelsize=20)\n",
    "# plt.tick_params(axis='y', labelsize=20)\n",
    "# ax.legend(loc='best', prop={'size': 20})\n",
    "# plt.xlabel(\"Spatial Frequency\", fontsize=20)\n",
    "# plt.ylabel(\"Power Spectrum\", fontsize=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2500, Total labels: 2500\n",
      "Training data shape : (2000, 722) (2000,)\n",
      "Training data shape : (500, 722) (500,)\n",
      "Train label=0 : 1000\n",
      "Train label=1 : 1000\n",
      "Test label=0 : 250\n",
      "Test label=1 : 250\n"
     ]
    }
   ],
   "source": [
    "# read python dict back from the file\n",
    "pkl_file = open(datadir, 'rb')\n",
    "data = pickle.load(pkl_file)\n",
    "\n",
    "pkl_file.close()\n",
    "images = data[\"data\"]\n",
    "labels = data[\"label\"]\n",
    "print(f\"Total images: {len(images)}, Total labels: {len(labels)}\")\n",
    "\n",
    "# Random Split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size = 0.2)\n",
    "\n",
    "# Stratified Split\n",
    "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "stratSplit.get_n_splits(images, labels)\n",
    "\n",
    "for train_index, test_index in stratSplit.split(images, labels):\n",
    "    X_train, X_test = images[train_index], images[test_index]\n",
    "    Y_train, Y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "print(\"Training data shape :\", X_train.shape, Y_train.shape)\n",
    "print(\"Training data shape :\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Train label=0 :\", np.count_nonzero(Y_train==0))\n",
    "print(\"Train label=1 :\", np.count_nonzero(Y_train==1))\n",
    "print(\"Test label=0 :\",  np.count_nonzero(Y_test==0))\n",
    "print(\"Test label=1 :\",  np.count_nonzero(Y_test==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method-SVC completed!\n",
      "Method MLP completed!\n",
      "Method-KNN completed!\n",
      "=========================\n",
      "MODEL ARCH.\t ACCURACY\n",
      "-------------------------\n",
      "SVM\t\t   100.0\n",
      "-------------------------\n",
      "MLP\t\t    95.0\n",
      "-------------------------\n",
      "knn_acc\t\t   100.0\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "def train_and_tune(X, y, model, parameters, scoring='f1_macro', kfold=5, verbose=0):\n",
    "    \"\"\"\n",
    "    X:          array-like of shape (n_samples, n_features)\n",
    "    y:          array-like of shape (n_samples,)\n",
    "    model:      (object) a sklearn model class\n",
    "    parameters: (dict) contains the parameters you want to tune in the model\n",
    "    metric:     (str) the metric used to evaluate the quality of the model\n",
    "    return:     a trained model with the best parameters\n",
    "    \"\"\"\n",
    "    cvSearchObj = GridSearchCV(model,\n",
    "                               parameters,\n",
    "                               scoring=scoring,\n",
    "                               n_jobs=-1,\n",
    "                               cv=kfold,\n",
    "                               verbose=verbose)\n",
    "    cvSearchObj.fit(X,y)\n",
    "    return cvSearchObj.best_estimator_\n",
    "\n",
    "\n",
    "def save_model(filename, model):\n",
    "    \"\"\"\n",
    "    filename: Filename to save the model\n",
    "    model:    Model weights to be saved\n",
    "    \"\"\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "    \n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: Filename to load the model\n",
    "    return:   Model weights that are reloaded\n",
    "    \"\"\"\n",
    "    model_reloaded = pickle.load(open(filename, 'rb'))\n",
    "    return model_reloaded\n",
    "\n",
    "\n",
    "def SupportVectorMachine(train, save, test):\n",
    "    filename = \"./checkpoint/svcBest.pt\"\n",
    "    svc = svm.SVC(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"kernel\":('linear', 'rbf'), \n",
    "                \"C\":[1, 10, 500, 1000]\n",
    "                }\n",
    "\n",
    "        svcBest = train_and_tune(X_train,\n",
    "                                 Y_train,\n",
    "                                 svc,\n",
    "                                 params,\n",
    "                                 scoring='f1_macro',\n",
    "                                 kfold=5)\n",
    "        if save:\n",
    "            save_model(filename, svcBest)\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        svcBest_reloaded = load_model(filename)\n",
    "        pred = svcBest_reloaded.predict(X_test)\n",
    "        acc  = svcBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix)*10, index = [i for i in classes],\n",
    "                             columns = [i for i in classes])\n",
    "        plt.figure(figsize = (12,10))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method-SVC completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "\n",
    "def MultiLayerPerceptron(train, save, test):\n",
    "    filename = \"./checkpoint/mlpBest.pt\"\n",
    "    mlp = neural_network.MLPClassifier(random_state=999)\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\n",
    "                   \"alpha\" : [0.0001],\n",
    "                   \"learning_rate_init\" : [0.005],\n",
    "                   \"batch_size\" : [8, 32, 64, 128],\n",
    "                   \"activation\" : [\"relu\"],\n",
    "                   \"early_stopping\" : [True],\n",
    "                   \"hidden_layer_sizes\" : [3, 10, 50, 100],\n",
    "                 }\n",
    "\n",
    "        mlpBest = train_and_tune(X_train,\n",
    "                                Y_train,\n",
    "                                mlp,\n",
    "                                params,\n",
    "                                scoring='f1_macro',\n",
    "                                kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, mlpBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        mlpBest_reloaded = load_model(filename)\n",
    "        pred = mlpBest_reloaded.predict(X_test)\n",
    "        acc  = mlpBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "                             columns = [i for i in classes])\n",
    "        plt.figure(figsize = (12,10))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method MLP completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "\n",
    "def KNearestNeighbors(train, save, test):\n",
    "    filename = \"./checkpoint/knnclassifierBest.pkl\"\n",
    "    knnclassifier = KNeighborsClassifier()\n",
    "    if train:\n",
    "        '''\n",
    "        Train\n",
    "        '''\n",
    "        params = {\"n_neighbors\": [2, 6, 10],\n",
    "                }\n",
    "\n",
    "        knnclassifierBest = train_and_tune(X_train,\n",
    "                                Y_train,\n",
    "                                knnclassifier,\n",
    "                                params,\n",
    "                                scoring='f1_macro',\n",
    "                                kfold=5)\n",
    "\n",
    "        if save:\n",
    "            save_model(filename, knnclassifierBest)\n",
    "\n",
    "    if test:\n",
    "        '''\n",
    "        Test\n",
    "        '''\n",
    "        knnclassifierBest_reloaded = load_model(filename)\n",
    "        pred = knnclassifierBest_reloaded.predict(X_test)\n",
    "        acc  = knnclassifierBest_reloaded.score(X_test, Y_test)\n",
    "        \n",
    "        cf_matrix = confusion_matrix(Y_test, pred)\n",
    "        df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "                             columns = [i for i in classes])\n",
    "        plt.figure(figsize = (12,10))\n",
    "        sn.heatmap(df_cm, annot=True)\n",
    "        \n",
    "        # print(\"Accuracy: \", 100*acc)\n",
    "    print(\"Method-KNN completed!\")\n",
    "    return 100*acc\n",
    "\n",
    "svm_acc = SupportVectorMachine(train=False, save=False, test=True)\n",
    "# mlp_acc = MultiLayerPerceptron(train=False, save=False, test=True)\n",
    "# knn_acc = KNearestNeighbors(train=False, save=False, test=True)\n",
    "\n",
    "print(\"=\"*25)\n",
    "print(\"MODEL ARCH.\\t ACCURACY\")\n",
    "print(\"-\"*25)\n",
    "print(\"SVM\\t\\t  \", svm_acc)\n",
    "print(\"-\"*25)\n",
    "print(\"MLP\\t\\t   \", mlp_acc)\n",
    "print(\"-\"*25)\n",
    "print(\"knn_acc\\t\\t  \", knn_acc)\n",
    "print(\"=\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}